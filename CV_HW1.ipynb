{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1pPxNXL75ZdV87rYS1UeNW98ZSIjmUnLb",
      "authorship_tag": "ABX9TyPOIp980p8Ba/fYQQx1SnRP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xoxonut/nycu-cv-113-2/blob/main/CV_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf \"/content/drive/MyDrive/hw1-data.tar.gz\" -C \"/content\" > /dev/null 2>&1\n",
        "!du -sh \"/content/data\"\n",
        "!awk 'BEGIN {sum=0} /^[0-9]+$/ {sum+=$1} END {print sum}' <(find \"/content/data/train\" -type f | wc -l) <(find \"/content/data/val\" -type f | wc -l)"
      ],
      "metadata": {
        "id": "545JUVYt90Bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee42cdb7-7b68-4932-804f-4e85dc2fedf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.5G\t/content/data\n",
            "21024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torchvision.models import resnet50,ResNet50_Weights\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch import nn\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import PIL.Image as Image\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "-bKg2bCHly22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms():\n",
        "  train_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  test_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "    transforms.Resize(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  return train_transform,test_transform\n"
      ],
      "metadata": {
        "id": "1YiI-rbOohTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(train_transform,test_transform,batch_size):\n",
        "  train_set = datasets.ImageFolder(\"/content/data/train\",transform=train_transform)\n",
        "  val_set = datasets.ImageFolder(\"/content/data/val\",transform=test_transform)\n",
        "  train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  class TestDataSet(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "      self.root_dir = root_dir\n",
        "      self.transform = transform\n",
        "      valid_exts = (\".jpg\",\".jpeg\",\".png\",\".bmp\")\n",
        "      self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.lower().endswith(valid_exts)]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image_path = self.image_paths[index]\n",
        "      image = Image.open(image_path).convert(\"RGB\")\n",
        "      if self.transform:\n",
        "        image = self.transform(image)\n",
        "      file_name = os.path.basename(image_path)\n",
        "      name_only = os.path.splitext(file_name)[0]\n",
        "      return image,name_only\n",
        "  test_set = TestDataSet(\"/content/data/test\",transform=test_transform)\n",
        "  test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader,val_loader,test_loader"
      ],
      "metadata": {
        "id": "XxVYB8cBok4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model,train_loader,optimizer,criterion,device):\n",
        "  model.train()\n",
        "  total_loss, correct, total = 0, 0, 0\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "  avg_loss = total_loss / len(train_loader)\n",
        "  avg_acc = correct / total * 100\n",
        "  return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "B7jaV7ANqFes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model,val_loader,criterion,device):\n",
        "  model.eval()\n",
        "  total_loss, correct, total = 0, 0, 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      predicted = torch.argmax(outputs, dim=1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "  avg_loss = total_loss / len(val_loader)\n",
        "  avg_acc = correct / total * 100\n",
        "  return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "xWc6_O6ornPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model,test_loader,device):\n",
        "  model.eval()\n",
        "  correct, total = 0, 0\n",
        "  predictions = []\n",
        "  with torch.no_grad():\n",
        "    for inputs, file_names in test_loader:\n",
        "      inputs = inputs.to(device)\n",
        "      outputs = model(inputs)\n",
        "      preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "      for file_name, pred in zip(file_names, preds):\n",
        "        predictions.append({\"image_name\": file_name,\n",
        "                            \"pred_label\": pred.item()})\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "4RMbQx9fr_i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def init_RestNet50(num_classes):\n",
        "#   model = torch.load(\"/content/drive/MyDrive/full_model.pth\")\n",
        "#   return model\n",
        "def init_RestNet50(num_classes):\n",
        "  model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "  num_ftrs = model.fc.in_features\n",
        "  model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, num_classes)\n",
        "  )\n",
        "  for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "  for p in model.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "  for p in model.layer4.parameters():\n",
        "    p.requires_grad = True\n",
        "  return model\n",
        "# class TenCropResNet50_3D(nn.Module):\n",
        "#   def __init__(self,num_classes):\n",
        "#     super(TenCropResNet50_3D,self).__init__()\n",
        "#     self.resnet50 = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "#     num_ftrs = self.resnet50.fc.in_features\n",
        "#     self.resnet50.fc = nn.Sequential(\n",
        "#       nn.Linear(num_ftrs, 512),\n",
        "#       nn.ReLU(),\n",
        "#       nn.Dropout(0.5),\n",
        "#       nn.Linear(512, num_classes)\n",
        "#     )\n",
        "#     for param in self.resnet50.parameters():\n",
        "#       param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "IvI2p675ntpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_accuracy(train_losses, val_losses, train_accs, val_accs):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accs, label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "K2jbIxK2vOuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_size(model):\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())  # in bytes\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())  # in bytes\n",
        "    total_size = (param_size + buffer_size) / (1024 ** 2)  # Convert to MB\n",
        "    print(f\"Model size: {total_size:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "T35AMw9OvacU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  train_transform,test_transform = get_transforms()\n",
        "  train_loader,val_loader,test_loader = create_dataloader(train_transform,test_transform,32)\n",
        "  model = init_RestNet50(100)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,weight_decay=0.01)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
        "  epochs = 10\n",
        "  train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
        "  print_model_size(model)\n",
        "  for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    torch.save(model,\"/content/drive/MyDrive/full_model.pth\")\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.2f}% - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.2f}%\")\n",
        "  plot_loss_accuracy(train_losses, val_losses, train_accs, val_accs)\n",
        "  res = test_model(model,test_loader,device)\n",
        "  df = pd.DataFrame(res)\n",
        "  df.to_csv(\"/content/drive/MyDrive/prediction.csv\",index=False)\n"
      ],
      "metadata": {
        "id": "6yOAAexJk2iM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}